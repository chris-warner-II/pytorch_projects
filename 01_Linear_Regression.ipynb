{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b807351",
   "metadata": {},
   "source": [
    "# Linear Regression \n",
    "\n",
    "This notebook will construct data for regression problems (both linear and non-linear data) using scikit-learn datasets. It will then train linear regression models and regression models with non-linear pointwise activation functions on that data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35616ef0",
   "metadata": {},
   "source": [
    "## 0. Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401b866d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Statements\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from sklearn.datasets import make_regression, make_friedman1\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from helper_functions import plot_predictions, plot_loss\n",
    "from models import SingleLayerLinearModel, ThreeLayerModel, train_step_regression, test_step_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf09a43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device is {device}\")\n",
    "\n",
    "# random number seed\n",
    "rns=42\n",
    "\n",
    "# Flag to make plots and print out things.\n",
    "verbose = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7fa747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, \n",
    "               loss_count, \n",
    "               test_loss_count, \n",
    "               datatype,\n",
    "               hid_dim,\n",
    "               nl_type,\n",
    "               num_layers):\n",
    "    \"\"\"\n",
    "    Return model name, best loss and best test_loss in a dictionary to compare models at the end.\n",
    "    \"\"\"\n",
    "\n",
    "    return {\"model_name\": model.__class__.__name__,\n",
    "            \"model_nonlin\": nl_type,\n",
    "            \"model_layers\": num_layers,\n",
    "            \"model_hidden_units\": hid_dim,\n",
    "            \"data\": datatype,\n",
    "            \"train_loss\": np.array(loss_count).min(),\n",
    "            \"test_loss\": np.array(test_loss_count).min()\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc763427",
   "metadata": {},
   "source": [
    "## 1. Single Layer Linear model on Linear data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea17ab7",
   "metadata": {},
   "source": [
    "### 1a. Generate Linear Data: train & test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4452734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear data\n",
    "\n",
    "dims=3\n",
    "X, y, coef = make_regression(n_samples=100, n_features=dims, noise=0, bias=5, coef=True, random_state=rns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d69782",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X is shape: {X.shape} and type: {type(X)}\")\n",
    "print(f\"y is shape: {y.shape} and type: {type(y)}\")\n",
    "print(f\"Ground truth coefficients: {coef}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8192c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train & test sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=rns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a38d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a115b835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numpy into torch tensors and put them onto device\n",
    "\n",
    "X_train = torch.from_numpy(X_train).type(torch.float32).to(device)\n",
    "y_train = torch.from_numpy(y_train).type(torch.float32).to(device)\n",
    "X_test = torch.from_numpy(X_test).type(torch.float32).to(device)\n",
    "y_test = torch.from_numpy(y_test).type(torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1316c86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12df171c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make y_train & y_test dimensions to be same as what model will predict - ie model0(X).\n",
    "\n",
    "y_train = y_train.unsqueeze(dim=1)\n",
    "y_test = y_test.unsqueeze(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2d58e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501a5395",
   "metadata": {},
   "source": [
    "### 1b. Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e45a3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data \n",
    "\n",
    "if verbose:\n",
    "    fig = plt.figure(figsize=(10,3))\n",
    "    for dim in range(dims):\n",
    "\n",
    "        plt.subplot(1, dims, dim+1)\n",
    "        plot_predictions(train_data = X_train[:,dim],\n",
    "                        train_labels = y_train,\n",
    "                        test_data = X_test[:,dim],\n",
    "                        test_labels = y_test\n",
    "                        )\n",
    "        fig.subplots_adjust(top=0.85)\n",
    "        plt.title(f'Dim {dim}')\n",
    "    fig.suptitle('Linear Data generated from make_regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c62e0d",
   "metadata": {},
   "source": [
    "### 1c. Build and instantiate linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57587448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SingleLayerLinearModel defined in models.py\n",
    "#\n",
    "# SingleLayerLinearModel is a single linear layer with 'in_dim' weights and 1 bias parameter \n",
    "# to fit a line in 'in_dim' dimensions. Should work for linear regression and fail when data becomes\n",
    "# more non-linear.\n",
    "\n",
    "# instantiate model\n",
    "hid_dim=None\n",
    "nl_type=None\n",
    "\n",
    "model0 = SingleLayerLinearModel(in_dim=dims).to(device) # single linear layer with 'dims' weight terms & 1 bias term.\n",
    "model_name = model0.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef997595",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Randomly initialized, model0 looks like: \\n {model0.state_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bbbfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with untrained model and visualize\n",
    "if verbose:\n",
    "    with torch.inference_mode():\n",
    "        preds = model0(X_test)\n",
    "\n",
    "    fig = plt.figure(figsize=(12,4))\n",
    "    for dim in range(dims):\n",
    "        plt.subplot(1, dims, dim+1)\n",
    "        plot_predictions(train_data = X_train[:,dim],\n",
    "                        train_labels = y_train,\n",
    "                        test_data = X_test[:,dim],\n",
    "                        test_labels = y_test,\n",
    "                        predictions = preds\n",
    "                        )\n",
    "        fig.subplots_adjust(top=0.85)\n",
    "        plt.title(f\"Dim {dim}\")\n",
    "    fig.suptitle(f\"Predictions made with untrained model0: {model_name}\",fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b04d63",
   "metadata": {},
   "source": [
    "### 1d. Train linear model on linear data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3762d8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup loss function and optimizer\n",
    "\n",
    "# For linear regression problems, can use mean squared error (MSELoss) or mean absolute error (L1Loss)\n",
    "loss_fn = nn.L1Loss() \n",
    "# loss_fn = nn.MSELoss() \n",
    "\n",
    "# Set optimizer to be Stochastic Gradient Descent and pass in model parameters.\n",
    "optimizer = torch.optim.SGD(params = model0.parameters(),\n",
    "                            lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817afd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup lists to gather loss through training\n",
    "epoch_count = []\n",
    "loss_count = []\n",
    "test_loss_count = []\n",
    "\n",
    "epochs = 3000 # how many times to run training loop - run through all data and adjust model params.\n",
    "test_freq = 100 # how often to run test step and print out, collect up results to track progress\n",
    "\n",
    "# 1. Loop through all data\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # 2. Training step\n",
    "    loss = train_step_regression(data = X_train,\n",
    "                                 labels = y_train,\n",
    "                                 model = model0,\n",
    "                                 loss_fn = loss_fn,\n",
    "                                 optimizer = optimizer)\n",
    "    \n",
    "    # 3. print what's happening at test_freq\n",
    "    if epoch % test_freq == 0:\n",
    "        \n",
    "        # 4. Test step\n",
    "        test_loss = test_step_regression(data = X_test,\n",
    "                                         labels = y_test,\n",
    "                                         model = model0,\n",
    "                                         loss_fn = loss_fn) \n",
    "        \n",
    "        # Collect performance into lists\n",
    "        epoch_count.append(epoch)\n",
    "        loss_count.append(loss.item())\n",
    "        test_loss_count.append(test_loss.item())\n",
    "        \n",
    "        # Print performance\n",
    "        print(f\"Epoch: {epoch:5d} | loss: {loss:.6f} | test loss: {test_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab5a600",
   "metadata": {},
   "source": [
    "### 1e. Visualize results: Linear regression model predicting on linear data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e305fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss\n",
    "if verbose:\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plot_loss(epoch = epoch_count,\n",
    "             loss = loss_count,\n",
    "             test_loss = test_loss_count,\n",
    "             y_scale='linear')\n",
    "    plt.title(f\"Loss during model0 {model_name} training\",fontsize=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8d10ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with trained model and visualize\n",
    "\n",
    "if verbose:\n",
    "    with torch.inference_mode():\n",
    "        preds = model0(X_test)\n",
    "\n",
    "    fig = plt.figure(figsize=(12,4))\n",
    "    for dim in range(dims):\n",
    "        plt.subplot(1, dims, dim+1)\n",
    "        plot_predictions(train_data = X_train[:,dim],\n",
    "                        train_labels = y_train,\n",
    "                        test_data = X_test[:,dim],\n",
    "                        test_labels = y_test,\n",
    "                        predictions = preds\n",
    "                        )\n",
    "        fig.subplots_adjust(top=0.85)\n",
    "        plt.title(f\"Dim: {dim}\")\n",
    "    fig.suptitle(f\"Predictions made by trained model0: {model_name}\",fontsize=16)\n",
    "    print(f\"Model 0 = \\n {model0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3371f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model after training: {model0.state_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8592559f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Ground truth coefficients were: {coef}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0304b356",
   "metadata": {},
   "outputs": [],
   "source": [
    "model0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0adcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "results0 = eval_model(model=model0,\n",
    "                      loss_count=loss_count,\n",
    "                      test_loss_count=test_loss_count,\n",
    "                      datatype=\"linear\",\n",
    "                      hid_dim=hid_dim,\n",
    "                      nl_type=nl_type,\n",
    "                      num_layers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a280d33a",
   "metadata": {},
   "source": [
    "**Conclusion**: Our single layer Linear Regression model, model0, does well predicting linear data. Model predictions, red x, above lie right on top of ground truth, green o, for test data - data that model never saw during training. Also, the weights learned by the model match very closely the ground truth coefficients used to train the model. **But, how will it do on non-linear data?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c5c30c",
   "metadata": {},
   "source": [
    "## 2. Single Layer Linear Model on Non-linear data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f59295a",
   "metadata": {},
   "source": [
    "### 2a. Generate Nonlinear data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2cb604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate nonlinear data - from Scikit Learn's Friedman1 generator.\n",
    "#\n",
    "# y(X) = 10 * sin(pi * X[:, 0] * X[:, 1]) +\n",
    "#        20 * (X[:, 2] - 0.5) ** 2 + \n",
    "#        10 * X[:, 3] + 5 * X[:, 4] + \n",
    "#        noise * N(0, 1).\n",
    "#\n",
    "# Note: Only the 1st 5 dimensions contribute to the nonlinear signal. Dimensions beyond 5 do not contribute to y.\n",
    "\n",
    "dims=5\n",
    "Xnl, ynl = make_friedman1(n_samples=500, n_features=dims, noise=0, random_state=rns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065d8738",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X is shape: {Xnl.shape} and type: {type(Xnl)}\")\n",
    "print(f\"y is shape: {ynl.shape} and type: {type(ynl)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6201e737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train & test sets\n",
    "\n",
    "Xnl_train, Xnl_test, ynl_train, ynl_test = train_test_split(Xnl, ynl, test_size=0.2, random_state=rns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848d3b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numpy into torch tensors and put them onto device\n",
    "\n",
    "Xnl_train = torch.from_numpy(Xnl_train).type(torch.float32).to(device)\n",
    "ynl_train = torch.from_numpy(ynl_train).type(torch.float32).to(device)\n",
    "Xnl_test = torch.from_numpy(Xnl_test).type(torch.float32).to(device)\n",
    "ynl_test = torch.from_numpy(ynl_test).type(torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b40436",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xnl_train.shape, Xnl_test.shape, ynl_train.shape, ynl_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15f7fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make ynl_train & ynl_test dimensions to be same as what model will predict - ie model(X).\n",
    "\n",
    "ynl_train = ynl_train.unsqueeze(dim=1)\n",
    "ynl_test = ynl_test.unsqueeze(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2760d2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xnl_train.shape, Xnl_test.shape, ynl_train.shape, ynl_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c2097e",
   "metadata": {},
   "source": [
    "### 2b. Visualize nonlinear data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfed997",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize data \n",
    "\n",
    "if verbose:\n",
    "    fig = plt.figure(figsize=(14,3))\n",
    "    rows=1\n",
    "    for dim in range(dims):\n",
    "        plt.subplot(rows,int(np.ceil(dims/rows)),dim+1)\n",
    "        plot_predictions(train_data = Xnl_train[:,dim],\n",
    "                        train_labels = ynl_train,\n",
    "                        test_data = Xnl_test[:,dim],\n",
    "                        test_labels = ynl_test\n",
    "                        )\n",
    "        fig.subplots_adjust(top=0.75)\n",
    "        plt.title(f\"Dim {dim}\")\n",
    "    fig.suptitle('Nonlinear data made by make_friedman1',fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd03073",
   "metadata": {},
   "source": [
    "### 2c. Build and instantiate linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9897ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinearRegressionModelV0 defined in models.py\n",
    "#\n",
    "# LinearRegressionModelV0 is a single linear layer with 'in_dim' weights and 1 bias parameter \n",
    "# to fit a line in 'in_dim' dimensions. Should work for linear regression and fail when data becomes\n",
    "# more non-linear.\n",
    "\n",
    "# instantiate model\n",
    "hid_dim=None\n",
    "nl_type=None\n",
    "\n",
    "model1 = SingleLayerLinearModel(in_dim=dims).to(device) # single linear layer with 'dims' weight terms & 1 bias term.\n",
    "model_name = model1.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5de03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Randomly initialized, model1 looks like: \\n {model1.state_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc9bc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with untrained model and visualize\n",
    "with torch.inference_mode():\n",
    "    preds = model1(Xnl_test)\n",
    "    \n",
    "fig = plt.figure(figsize=(14,3))\n",
    "rows=1\n",
    "for dim in range(dims):\n",
    "    plt.subplot(1, int(np.ceil(dims/1)), dim+1)\n",
    "    plot_predictions(train_data = Xnl_train[:,dim],\n",
    "                    train_labels = ynl_train,\n",
    "                    test_data = Xnl_test[:,dim],\n",
    "                    test_labels = ynl_test,\n",
    "                    predictions = preds\n",
    "                    )\n",
    "    fig.subplots_adjust(top=0.75)\n",
    "    plt.title(f\"Dim {dim}\")\n",
    "fig.suptitle(f\"Predictions made with untrained model1: {model_name}\",fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbc5d3d",
   "metadata": {},
   "source": [
    "### 1d. Train a single layer linear model on nonlinear data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629e9d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup loss function and optimizer\n",
    "\n",
    "# loss_fn = nn.L1Loss() \n",
    "loss_fn = nn.MSELoss() \n",
    "\n",
    "optimizer = torch.optim.SGD(params = model1.parameters(),\n",
    "                            lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a391ed5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "# Setup lists to gather loss through training\n",
    "epoch_count = []\n",
    "loss_count = []\n",
    "test_loss_count = []\n",
    "\n",
    "epochs = 3000\n",
    "test_freq = 300\n",
    "\n",
    "# 1. Loop through all data\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # 2. Training step\n",
    "    loss = train_step_regression(data = Xnl_train,\n",
    "                                 labels = ynl_train,\n",
    "                                 model = model1,\n",
    "                                 loss_fn = loss_fn,\n",
    "                                 optimizer = optimizer)\n",
    "    \n",
    "    # 3. print what's happening at test_freq\n",
    "    if epoch % test_freq == 0:\n",
    "        \n",
    "        # 4. Test step\n",
    "        test_loss = test_step_regression(data = Xnl_test,\n",
    "                                         labels = ynl_test,\n",
    "                                         model = model1,\n",
    "                                         loss_fn = loss_fn) \n",
    "        \n",
    "        # Collect performance into lists\n",
    "        epoch_count.append(epoch)\n",
    "        loss_count.append(loss.item())\n",
    "        test_loss_count.append(test_loss.item())\n",
    "        \n",
    "        # Print performance\n",
    "        print(f\"Epoch: {epoch:5d} | loss: {loss:.6f} | test loss: {test_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fb067a",
   "metadata": {},
   "source": [
    "### 2e. Visualize results: Linear regression model predicting on nonlinear data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e093871c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss\n",
    "if verbose:\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plot_loss(epoch = epoch_count,\n",
    "             loss = loss_count,\n",
    "             test_loss = test_loss_count,\n",
    "             y_scale='log')\n",
    "    plt.title(f\"Loss during training model1 {model_name} on nonlinear data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2f280c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with trained model and visualize\n",
    "if verbose:\n",
    "    with torch.inference_mode():\n",
    "        preds = model1(Xnl_test)\n",
    "        \n",
    "    fig=plt.figure(figsize=(12,3))\n",
    "    rows=1\n",
    "    for dim in range(dims):\n",
    "        plt.subplot(rows,int(np.ceil(dims/rows)),dim+1)\n",
    "        plot_predictions(train_data = Xnl_train[:,dim],\n",
    "                        train_labels = ynl_train,\n",
    "                        test_data = Xnl_test[:,dim],\n",
    "                        test_labels = ynl_test,\n",
    "                        predictions = preds\n",
    "                        )\n",
    "        fig.subplots_adjust(top=0.75)\n",
    "        plt.title(f\"Dim {dim}\")\n",
    "    fig.suptitle(f\"Predictions made on Nonlinear data by trained model1: {model_name}\")\n",
    "    print(f\"Model 1 = \\n {model1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d9b045",
   "metadata": {},
   "outputs": [],
   "source": [
    "results1 = eval_model(model=model1,\n",
    "                      loss_count=loss_count,\n",
    "                      test_loss_count=test_loss_count,\n",
    "                      datatype=\"nonlinear\",\n",
    "                      hid_dim=hid_dim,\n",
    "                      nl_type=nl_type,\n",
    "                      num_layers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cd9cd0",
   "metadata": {},
   "source": [
    "**Conclusion:** Single layer linear model Model does learn on non-linear data but loss bottoms out and we can see predictions (red xs) dont match ground truth (green os) that well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d95f52",
   "metadata": {},
   "source": [
    "## 3. Three Layer Linear Model on non-linear data\n",
    "\n",
    "Using the same non-linear data from make_freidman1, we will train a 3 layer linear model on it to see if that does better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef462a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models built / defined in models.py\n",
    "\n",
    "# instantiate model\n",
    "hid_dim=10\n",
    "nl_type=None\n",
    "\n",
    "model2 = ThreeLayerModel(in_dim=dims,\n",
    "                               hid_dim=hid_dim,\n",
    "                               nl_type=nl_type).to(device) \n",
    "model_name = model2.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fe1db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb4a1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Randomly initialized, model2 looks like: \\n {model2.state_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9de932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with untrained model and visualize\n",
    "\n",
    "with torch.inference_mode():\n",
    "    preds = model2(Xnl_test)\n",
    "    \n",
    "fig = plt.figure(figsize=(14,3))\n",
    "rows=1\n",
    "for dim in range(dims):\n",
    "    plt.subplot(1, int(np.ceil(dims/1)), dim+1)\n",
    "    plot_predictions(train_data = Xnl_train[:,dim],\n",
    "                    train_labels = ynl_train,\n",
    "                    test_data = Xnl_test[:,dim],\n",
    "                    test_labels = ynl_test,\n",
    "                    predictions = preds\n",
    "                    )\n",
    "    fig.subplots_adjust(top=0.75)\n",
    "    plt.title(f\"Dim {dim}\")\n",
    "fig.suptitle(f\"Predictions made with untrained model2: {model_name}\",fontsize=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f88b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup loss function and optimizer\n",
    "\n",
    "# loss_fn = nn.L1Loss() \n",
    "loss_fn = nn.MSELoss() \n",
    "\n",
    "optimizer = torch.optim.SGD(params = model2.parameters(),\n",
    "                            lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9460c4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "# Setup lists to gather loss through training\n",
    "epoch_count = []\n",
    "loss_count = []\n",
    "test_loss_count = []\n",
    "\n",
    "epochs = 3000\n",
    "test_freq = 300\n",
    "\n",
    "# 1. Loop through all data\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # 2. Training step\n",
    "    loss = train_step_regression(data = Xnl_train,\n",
    "                                 labels = ynl_train,\n",
    "                                 model = model2,\n",
    "                                 loss_fn = loss_fn,\n",
    "                                 optimizer = optimizer)\n",
    "    \n",
    "    # 3. print what's happening at test_freq\n",
    "    if epoch % test_freq == 0:\n",
    "        \n",
    "        # 4. Test step\n",
    "        test_loss = test_step_regression(data = Xnl_test,\n",
    "                                         labels = ynl_test,\n",
    "                                         model = model2,\n",
    "                                         loss_fn = loss_fn) \n",
    "        \n",
    "        # Collect performance into lists\n",
    "        epoch_count.append(epoch)\n",
    "        loss_count.append(loss.item())\n",
    "        test_loss_count.append(test_loss.item())\n",
    "        \n",
    "        # Print performance\n",
    "        print(f\"Epoch: {epoch:5d} | loss: {loss:.6f} | test loss: {test_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cba955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss\n",
    "\n",
    "if verbose:\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plot_loss(epoch = epoch_count,\n",
    "             loss = loss_count,\n",
    "             test_loss = test_loss_count,\n",
    "             y_scale='log')\n",
    "    plt.title(f\"Loss during training model2 {model_name} on nonlinear data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a6f2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with trained model and visualize\n",
    "\n",
    "if verbose:\n",
    "    with torch.inference_mode():\n",
    "        preds = model2(Xnl_test)\n",
    "        \n",
    "    fig=plt.figure(figsize=(12,3))\n",
    "    rows=1\n",
    "    for dim in range(dims):\n",
    "        plt.subplot(rows,int(np.ceil(dims/rows)),dim+1)\n",
    "        plot_predictions(train_data = Xnl_train[:,dim],\n",
    "                        train_labels = ynl_train,\n",
    "                        test_data = Xnl_test[:,dim],\n",
    "                        test_labels = ynl_test,\n",
    "                        predictions = preds\n",
    "                        )\n",
    "        fig.subplots_adjust(top=0.75)\n",
    "        plt.title(f\"Dim {dim}\")\n",
    "    fig.suptitle(f\"Predictions made on Nonlinear data by trained linear model2 {model_name}\")\n",
    "    print(f\"Model 2 = \\n {model2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f640d09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results2 = eval_model(model=model2,\n",
    "                      loss_count=loss_count,\n",
    "                      test_loss_count=test_loss_count,\n",
    "                      datatype=\"nonlinear\",\n",
    "                      hid_dim=hid_dim,\n",
    "                      nl_type=nl_type,\n",
    "                      num_layers=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a63a0b",
   "metadata": {},
   "source": [
    "**Conclusion:** Three layer linear model does learn on non-linear data but loss bottoms out at basically where the single layer linear did. And the predictions it makes (red x's) dont match ground truth (green o's) any better.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0503f5",
   "metadata": {},
   "source": [
    "## 4. Three Layer Non-linear Model on non-linear data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5042f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models built / defined in models.py\n",
    "\n",
    "# instantiate model\n",
    "hid_dim=10\n",
    "nl_type='relu'\n",
    "\n",
    "model3 = ThreeLayerModel(in_dim=dims,\n",
    "                               hid_dim=hid_dim,\n",
    "                               nl_type=nl_type).to(device) \n",
    "model_name = model3.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d25dd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989619c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with untrained model and visualize\n",
    "\n",
    "with torch.inference_mode():\n",
    "    preds = model3(Xnl_test)\n",
    "    \n",
    "fig = plt.figure(figsize=(14,3))\n",
    "rows=1\n",
    "for dim in range(dims):\n",
    "    plt.subplot(1, int(np.ceil(dims/1)), dim+1)\n",
    "    plot_predictions(train_data = Xnl_train[:,dim],\n",
    "                    train_labels = ynl_train,\n",
    "                    test_data = Xnl_test[:,dim],\n",
    "                    test_labels = ynl_test,\n",
    "                    predictions = preds\n",
    "                    )\n",
    "    fig.subplots_adjust(top=0.75)\n",
    "    plt.title(f\"Dim {dim}\")\n",
    "fig.suptitle(f'Predictions made with untrained model3: {model_name}',fontsize=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ce50c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup loss function and optimizer\n",
    "\n",
    "# loss_fn = nn.L1Loss() \n",
    "loss_fn = nn.MSELoss() \n",
    "\n",
    "optimizer = torch.optim.SGD(params = model3.parameters(),\n",
    "                            lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5995a496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "# Setup lists to gather loss through training\n",
    "epoch_count = []\n",
    "loss_count = []\n",
    "test_loss_count = []\n",
    "\n",
    "epochs = 10000\n",
    "test_freq = 300\n",
    "\n",
    "# 1. Loop through all data\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # 2. Training step\n",
    "    loss = train_step_regression(data = Xnl_train,\n",
    "                                 labels = ynl_train,\n",
    "                                 model = model3,\n",
    "                                 loss_fn = loss_fn,\n",
    "                                 optimizer = optimizer)\n",
    "    \n",
    "    # 3. print what's happening at test_freq\n",
    "    if epoch % test_freq == 0:\n",
    "        \n",
    "        # 4. Test step\n",
    "        test_loss = test_step_regression(data = Xnl_test,\n",
    "                                         labels = ynl_test,\n",
    "                                         model = model3,\n",
    "                                         loss_fn = loss_fn) \n",
    "        \n",
    "        # Collect performance into lists\n",
    "        epoch_count.append(epoch)\n",
    "        loss_count.append(loss.item())\n",
    "        test_loss_count.append(test_loss.item())\n",
    "        \n",
    "        # Print performance\n",
    "        print(f\"Epoch: {epoch:5d} | loss: {loss:.6f} | test loss: {test_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e80bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss\n",
    "\n",
    "if verbose:\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plot_loss(epoch = epoch_count,\n",
    "             loss = loss_count,\n",
    "             test_loss = test_loss_count,\n",
    "             y_scale='log')\n",
    "    plt.title(f\"Loss during training of model3 {model_name} on nonlinear data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b485c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with trained model and visualize\n",
    "\n",
    "if verbose:\n",
    "    with torch.inference_mode():\n",
    "        preds = model3(Xnl_test)\n",
    "        \n",
    "    fig=plt.figure(figsize=(12,3))\n",
    "    rows=1\n",
    "    for dim in range(dims):\n",
    "        plt.subplot(rows,int(np.ceil(dims/rows)),dim+1)\n",
    "        plot_predictions(train_data = Xnl_train[:,dim],\n",
    "                        train_labels = ynl_train,\n",
    "                        test_data = Xnl_test[:,dim],\n",
    "                        test_labels = ynl_test,\n",
    "                        predictions = preds\n",
    "                        )\n",
    "        fig.subplots_adjust(top=0.75)\n",
    "        plt.title(f\"Dim {dim}\")\n",
    "    fig.suptitle(f\"Predictions made on Nonlinear data by trained linear model3: {model_name}\")\n",
    "    print(f\"Model 3 = \\n {model3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cfa85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results3 = eval_model(model=model3,\n",
    "                      loss_count=loss_count,\n",
    "                      test_loss_count=test_loss_count,\n",
    "                      datatype=\"nonlinear\",\n",
    "                      hid_dim=hid_dim,\n",
    "                      nl_type=nl_type,\n",
    "                      num_layers=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70884626",
   "metadata": {},
   "source": [
    "**Conclusion:** Three layer non-linear model does learn on non-linear data better than linear models. And the predictions it makes (red x's) are closer to ground truth (green o's). Still not perfect, but we can train for longer. We can also add more hidden units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c09b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame([results0,results1,results2,results3])\n",
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cd4b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"IM DONE!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ced808f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
